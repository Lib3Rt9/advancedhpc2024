Test 1 - Base implementation on CPU

Problem: Super slow!

Causes:
	- Distance Calculation in Loops: Calculating Euclidean distances in loops, especially in both KMeans and FuzzyCMeans, can become computationally heavy for images with many pixels. Each distance calculation is done individually in a loop, which is inefficient for large datasets.
	- Lack of Vectorization: The current implementation calculates distances for each point one at a time. Vectorized operations in libraries like NumPy can dramatically reduce processing time by performing batch computations.
	- Python Loops in predict and fit Methods: The predict and fit methods in both classes use Python loops, which are slower than vectorized operations for large datasets.
	- Membership Matrix in FuzzyCMeans: The membership matrix update step in Fuzzy C-Means is computationally expensive due to the fuzzy exponentiation and multiple distance computations. This step is a known bottleneck.


################################################

Test 2 - Optimization for Test 1

Solve Test 1:
	- Vectorize Euclidean Distance Calculations: To speed up KMeans and FuzzyCMeans, replace the _euclidean_distance method with a vectorized version that computes distances for all pixels in a single operation.
	- Avoid Python Loops with NumPy Broadcasting: Use NumPy broadcasting to assign each pixel to the nearest centroid in a single step, bypassing Python loops and boosting performance.


Test 2 is using vectorized calculation for calculating the distances between centroids and the points
	class Kmean is using Euclidean vectorize
	class FuzzyCMean is using NumPy's broadcasting and 'np.linalg.norm'


################################################

Test 3 - GPU implementation - based on Test 2

Problem:
	SOLVED - Not able to run method
	SOLVED - Warning divided by 0 when calculating new_centroids
	SOLVED - Quantized image has only 1 cluster/color
	- Class FuzzyCMeans not being updated yet

Causes:
	- problem with thread block x, y
	- some cluster has 0 points
	- error in handling NAN and Inf value at this line: `new_centroids = np.nan_to_num(new_centroids, nan=0, posinf=255, neginf=0)  # Handle NaNs and infinities`

Solution:
	- Use thread per block: (16, 16) instead of 256
	- Error handling - need at least 1 point per cluster
	- remove line for handling NAN and Inf


################################################

Test 4 - GPU implemetation for FuzzyCMeans
	- based on Test 3

Problem:
	- Warning dividing to 0 ?
	- NAN and Inf values causing quantized image has only 1 cluster. ?
	SOLVED - Quantize image with GPU causing only 1 color - all centroids are almost in the same position -> all in 1 cluster.	

Causes:
	- 1 color quantize: problem with calculating distances in membership matrix perhaps?

Solution:
	- ? update membership matrix with GPU along calculating centroids


################################################

Test 5 - GPU with shared memory
	- based on Test 4

Problem:
	SOLVED - Shared memory not working
	- Shared memory: The _calculate_centroids_gpu could better utilize shared memory
	- Shared memory: Fixed shared memory size (32x32) might not be optimal for all cases
	- Parallelization: Potential thread divergence in some loops
	- Parallelization: Atomic operations in _calculate_centroids_gpu could be a bottleneck


################################################

Test 6 - GPU optimization
	- based on Test 5

Promlem:
	- Thread divergence, as not all threads may be involved in each step depending on the grid size and the dimensions of X
	- If the dimensions of data are larger than the block size, the blocks will have underutilized threads
	- Using cuda.syncthreads() multiple times, which might slow down the performance
	- Atomic operations can cause contention if too many threads try to access the same memory locations simultaneously, leading to bottlenecks

Cause:
	- In _calculate_distances_gpu, iterating through X.shape[1] and loading data into shared memory within the loop
	- Using shared memory arrays for temporary calculations in _calculate_centroids_gpu and _update_centroids_gpu_fcm
	- Using cuda.syncthreads() more than it is needed (?)
	- Atomic operations -> bottleneck

Solution:
	- Not use if inside loop, load only necessary values into shared memory using thread coordination
	- Ensure number of threads and shared memory usage is optimized based on the dataset size
	- Use synchronization only when absolutely necessary -> limit cuda.syncthreads()
	- Minimize the number of atomic operations or use other ways